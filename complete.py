import os
import re
import io
import time
import zipfile
import concurrent.futures
from collections import defaultdict
from typing import Optional
from datetime import datetime

import fitz  # PyMuPDF
import pandas as pd
import streamlit as st
import plotly.graph_objects as go
from openai import OpenAI

# ================== OpenAI ì„¤ì • (secrets ìš°ì„ , ì—†ìœ¼ë©´ env) ==================
def get_openai_client() -> OpenAI:
    api_key = None
    try:
        if "OPENAI_API_KEY" in st.secrets:
            api_key = st.secrets["OPENAI_API_KEY"]
        elif "openai" in st.secrets and "api_key" in st.secrets["openai"]:
            api_key = st.secrets["openai"]["api_key"]
    except Exception:
        pass
    if not api_key:
        api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("âŒ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. secrets.toml ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        st.stop()
    return OpenAI(api_key=api_key)

client = get_openai_client()
MODEL_NAME = "gpt-4.1-mini"

# ================== ì „ì—­ ìƒìˆ˜/ë§¤í•‘ ==================
# ğŸ‘‰ ì›”â†’ë¶„ê¸° ë§¤í•‘ì‹œ 'F' ë¶€ì—¬ ê¸°ì¤€ (ìš”ì²­ ì‚¬í•­: MAR-25â†’1Q25, JUN-25â†’2Q25, SEP-25â†’3Q25F, DEC-25â†’4Q25F)
CURRENT_YEAR = 2025
LAST_ACTUAL_QUARTER = 2  # ê°™ì€ í•´ì—ì„œ ì´ ë¶„ê¸°ë³´ë‹¤ í° ë¶„ê¸°ëŠ” F ì²˜ë¦¬

# ì—´ ì œê±° í† í° (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ) - CHG ê´€ë ¨ í† í° ì¶”ê°€
EXCLUDE_COL_TOKENS = (
    "DELTA", "Î”", "CONSENSUS", "CONS.", "VS CONSENSUS", "%",
    "REVISED", "PREVIOUS", "CHG.", "CHG", "CHANGE", "2025E.1", "YR", "YR.1", "YR.2",
    "_CHG"  # ìƒˆë¡œ ì¶”ê°€: _CHG ì ‘ë¯¸ì‚¬ í¬í•¨ ì»¬ëŸ¼ ì œê±°
)

# ì œê±°í•  í–‰ íŒ¨í„´ (ì¶”ê°€ëœ FCF old, FCF Î”, GP old, GP Î”)
EXCLUDE_ROW_PATTERNS = [
    r"fcf\s*(old|Ã¢Ë†â€ |delta|Î”)",
    r"gp\s*(old|Ã¢Ë†â€ |delta|Î”)",
    r"gross\s*profit\s*(old|Ã¢Ë†â€ |delta|Î”)",
    r"free\s*cash\s*flow\s*(old|Ã¢Ë†â€ |delta|Î”)"
]

# ì§€í‘œëª… í‘œì¤€í™” (AMD ìš©ì–´ ì¶”ê°€)
index_rename_map = {
    "revenue": "Revenue", "net revenue": "Revenue", "total revenue": "Revenue",
    "cost of revenue": "COGS", "cogs": "COGS", "cost of sales": "COGS",
    "gross profit": "GP", "gp": "GP", "gross margin": "GM", "gm": "GM",
    "op": "OP", "operating income": "OP", "operating profit": "OP",
    "op margin": "OP margin", "operating margin": "OP margin",
    "ebitda": "EBITDA", "ebitda margin": "EBITDA margin",
    "net profit": "NP", "np": "NP", "net income": "NP",
    "net profit margin": "NP Margin", "net margin": "NP Margin",
    "revenue growth": "revenue growth",
    "eps": "EPS", "earnings per share": "EPS", "non-gaap eps": "EPS",
    "roe": "ROE", "return on equity": "ROE",
    "operating leverage": "operating leverage",
    "free cash flow": "FCF", "fcf": "FCF",
    "research and development": "R&D", "r&d": "R&D",
    "capex": "CapEx", "capital expenditure": "CapEx", "capital expenditures": "CapEx",
    "property and equipment": "PP&E", "pp&e": "PP&E",
}

# AMD íŠ¹í™” ì§€í‘œ ë§¤í•‘
amd_specific_mappings = {
    "data center": "revenue-Data Center",
    "client": "revenue-Client",
    "gaming": "revenue-Gaming",
    "embedded": "revenue-Embedded",
    "gpu": "revenue-GPU",
    "cpu": "revenue-CPU",
}

# (ì„ íƒ) ì¼ë¶€ ê¸°ì—… ì„¸ê·¸ë¨¼íŠ¸ ì‚¬ì „ â€” ìˆìœ¼ë©´ ìš°ì„  ì ìš©, ì—†ìœ¼ë©´ generic ê°ì§€ ì‚¬ìš©
company_segments = {
    "Nvidia": ["Data Center", "Gaming", "Pro Visualization", "Automotive", "OEM & Other"],
    "NVIDIA": ["Data Center", "Gaming", "Pro Visualization", "Automotive", "OEM & Other"],
    "google": ["Google Services", "Google Cloud", "Other Bets"],
    "Amazon": ["North America", "International", "AWS", "Advertising"],
    "Meta": ["Family of Apps", "Reality Labs"],
    "Microsoft": ["Productivity", "Intelligent Cloud", "Personal Computing"],
    "SK Hynix": ["DRAM", "NAND"],
    "Samsung Electronics": ["DX", "DS", "Display", "Harman"],
    "AMD": ["Data Center", "Client", "Gaming", "Embedded"],
    "Advanced Micro Devices": ["Data Center", "Client", "Gaming", "Embedded"],
}

# AMD í…œí”Œë¦¿ ë°ì´í„° (ì˜ˆì‹œ)
amd_template_data = {
    "Revenue": {
        2023: 22680, 2024: 25785, "2025F": 32659, "2026F": 38178,
        "1Q24": 5473, "2Q24": 5835, "3Q24": 6819, "4Q24": 7658,
        "1Q25": 7438, "2Q25": 7685, "3Q25F": 8738, "4Q25F": 8798
    },
    "COGS": {
        2023: 11244, 2024: 12026, "2025F": 15784, "2026F": 16879,
        "1Q24": 2612, "2Q24": 2734, "3Q24": 3162, "4Q24": 3518,
        "1Q25": 3446, "2Q25": 4359, "3Q25F": 4019, "4Q25F": 3959
    },
    "GP": {
        2023: 11436, 2024: 13759, "2025F": 16876, "2026F": 21299,
        "1Q24": 2861, "2Q24": 3101, "3Q24": 3657, "4Q24": 4140,
        "1Q25": 3992, "2Q25": 3326, "3Q25F": 4719, "4Q25F": 4839
    },
    "OP": {
        2023: 4834, 2024: 6138, "2025F": 7019, "2026F": 9994,
        "1Q24": 1133, "2Q24": 1264, "3Q24": 1715, "4Q24": 2026,
        "1Q25": 1779, "2Q25": 897, "3Q25F": 2169, "4Q25F": 2174
    },
    "NP": {
        2023: 4292, 2024: 5420, "2025F": 6142, "2026F": 8747,
        "1Q24": 1013, "2Q24": 1126, "3Q24": 1504, "4Q24": 1777,
        "1Q25": 1566, "2Q25": 781, "3Q25F": 1895, "4Q25F": 1900
    },
    "EPS": {
        2023: 2.64, 2024: 3.31, "2025F": 3.77, "2026F": 5.34,
        "1Q24": 0.62, "2Q24": 0.69, "3Q24": 0.92, "4Q24": 1.09,
        "1Q25": 0.96, "2Q25": 0.48, "3Q25F": 1.16, "4Q25F": 1.16
    }
}

# ================== ìœ í‹¸ ==================
def safe_filename(name: str) -> str:
    base = re.sub(r"\s+", "_", name.strip())
    base = re.sub(r"[^A-Za-z0-9ê°€-í£_.-]+", "_", base)
    return base

def strip_code_fences(s: str) -> str:
    s = s.strip()
    s = re.sub(r"^```[a-zA-Z0-9]*\n", "", s)
    s = re.sub(r"\n```$", "", s)
    return s.strip()

def split_multiple_tables(text: str) -> list[str]:
    if "\n\n" in text:
        return [t.strip() for t in text.split("\n\n") if t.strip()]
    if "---" in text:
        return [t.strip() for t in text.split("---") if t.strip()]
    return [text.strip()]

# ================== ê¸°ê°„ í—¤ë” ì •ê·œí™” ==================
_MONTH_TO_Q = {"JAN": 1, "FEB": 1, "MAR": 1, "APR": 2, "MAY": 2, "JUN": 2, "JUL": 3, "AUG": 3, "SEP": 3, "OCT": 4, "NOV": 4, "DEC": 4}

def _to_yyyy(y: str) -> str:
    y = y.strip()
    if len(y) == 2:
        return "20" + y
    return y

def _is_future_quarter(yyyy: int, q: int) -> bool:
    if yyyy > CURRENT_YEAR:
        return True
    if yyyy < CURRENT_YEAR:
        return False
    # ê°™ì€ í•´: ì‹¤ì œ ë°œí‘œëœ ë¶„ê¸°ë³´ë‹¤ í¬ë©´ ì˜ˆì¸¡(F)
    return q > LAST_ACTUAL_QUARTER

def get_summary_from_pdf(pdf_text, client, MODEL_NAME):
    prompt = f"""
ë„ˆëŠ” ì§€ê¸ˆ ê¸ˆìœµ ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì•¼. ì•„ë˜ëŠ” í•œ ê¸°ì—…ì— ëŒ€í•œ PDF ë³´ê³ ì„œ ì „ì²´ í…ìŠ¤íŠ¸ì•¼. ë‹¤ìŒ 3ê°€ì§€ ì‘ì—…ì„ ìˆ˜í–‰í•´ ì¤˜.

### 1. í•µì‹¬ ìš”ì•½
- ì €ìê°€ ë§í•˜ê³ ì í•˜ëŠ” í•µì‹¬ ë‚´ìš©ì„ **ë³´ê³ ì„œ ë‚´ì˜ ê·¼ê±°ë§Œì„ ë°”íƒ•ìœ¼ë¡œ 1ë¬¸ì¥**ìœ¼ë¡œ ìš”ì•½í•´ ì¤˜.
- ë§Œì•½ ë³´ê³ ì„œì—ì„œ ëª…í™•í•œ í•µì‹¬ ë‚´ìš©ì„ ì°¾ê¸° ì–´ë µë”ë¼ë„, ë°˜ë“œì‹œ "í•µì‹¬ìš”ì•½:" ì´ë¼ëŠ” í‚¤ì›Œë“œ ë‹¤ìŒì— ìš”ì•½ ë¬¸ì¥ì„ ì‘ì„±í•´ ì¤˜. ì ˆëŒ€ë¡œ ì´ í‚¤ì›Œë“œë¥¼ ëˆ„ë½í•˜ì§€ ë§ˆ.
- **ì ˆëŒ€ë¡œ ì¶”ë¡ ì´ë‚˜ ê°œì¸ì ì¸ ì˜ê²¬ì„ í¬í•¨í•˜ì§€ ë§ˆ.**

### 2. ì£¼ìš” ì§€í‘œ
- ë³´ê³ ì„œ ë‚´ í‘œ ë˜ëŠ” í…ìŠ¤íŠ¸ì—ì„œ **ëª…ì‹œëœ** ì•„ë˜ ë”•ì…”ë„ˆë¦¬ ì§€í‘œ 5ê°€ì§€ë¥¼ **ê°ê´€ì ì¸ íŒ©íŠ¸**ë¡œ ì‘ì„±í•´ ì¤˜.
- ì§€í‘œëª…, ì—°ë„(ì˜ˆ: 2022, 1Q25 ë“±), ìˆ˜ì¹˜, ë‹¨ìœ„ê°€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•´.
- **ë°˜ë“œì‹œ ì „ë…„/ì „ë¶„ê¸° ê°’ê³¼ ì¦ê°ë¥ ì„ í•¨ê»˜ ëª…ì‹œí•´ ì¤˜.**
- ì¶”ë¡ ì´ë‚˜ ì˜ˆì¸¡ì€ ì ˆëŒ€ ê¸ˆì§€í•˜ë©°, ë³´ê³ ì„œì—ì„œ ì§ì ‘ í™•ì¸ ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ ì‚¬ìš©í•´.

ë”•ì…”ë„ˆë¦¬:
- Revenue: revenue, ë§¤ì¶œ, ë§¤ì¶œì•¡, net sales
- Cost of Revenue: cost of revenue, cogs, ë§¤ì¶œì›ê°€
- Gross Profit: gross profit, gp, ë§¤ì¶œì´ì´ìµ
- Gross Margin: gross margin, gross profit margin
- Operating Profit: op, operating profit, ì˜ì—…ì´ìµ
- OP Margin: op margin, operating profit margin, ì˜ì—…ì´ìµë¥ 
- EBITDA: ebitda
- EBITDA Margin: ebitda margin
- Net Profit: np, net profit, ë‹¹ê¸°ìˆœì´ìµ
- Net Profit Margin: net profit margin
- Revenue Growth: revenue growth
- EPS: eps, earnings per share
- ROE: roe
- Operating Leverage: operating leverage
- FCF: fcf, free cash flow, ì‰ì—¬í˜„ê¸ˆíë¦„
- CapEx: capex, capital expenditure, ì„¤ë¹„íˆ¬ì

3. ê° í˜ì´ì§€ë³„ë¡œ ì´ìƒì¹˜ê°€ ìˆìœ¼ë©´, ì•„ë˜ ê¸°ì¤€ì— ë”°ë¼ ë”•ì…”ë„ˆë¦¬ì— ëª…ì‹œëœ ì£¼ìš”ì§€í‘œ ì¤‘ì—ì„œ ì´ìƒì¹˜ í•­ëª©ê³¼ í•´ë‹¹ í˜ì´ì§€ ë²ˆí˜¸, ì´ìƒì¹˜ ë°œìƒ ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
- ì´ìƒì¹˜ ê¸°ì¤€: ì „ë…„ ëŒ€ë¹„ ë˜ëŠ” ì „ ë¶„ê¸° ëŒ€ë¹„ 20% ì´ìƒ ì¦ê°, ë˜ëŠ” **ê°’ì´ 0ì´ê±°ë‚˜ ìŒìˆ˜ì¸ ê²½ìš°**
- ì´ìƒì¹˜ íŒë‹¨ì€ í‘œ ë‚´ ìˆ˜ì¹˜ì™€ í…ìŠ¤íŠ¸ ë‚´ ì„¤ëª…ì„ ê·¼ê±°ë¡œ í•©ë‹ˆë‹¤.
- **ì´ìƒì¹˜ ë°œìƒ ì›ì¸ì„ ë³´ê³ ì„œ ë‚´ì˜ êµ¬ì²´ì ì¸ í…ìŠ¤íŠ¸ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.**

ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì´ í•´ì£¼ì„¸ìš”.

í•µì‹¬ìš”ì•½: (1ë¬¸ì¥ í•µì‹¬ ìš”ì•½)

ì£¼ìš”ì§€í‘œ:

1. (ì—°ë„ì™€ ìˆ˜ì¹˜ê°€ ëª…í™•í•œ ê°ê´€ì  ì§€í‘œ 1)
2. (ì—°ë„ì™€ ìˆ˜ì¹˜ê°€ ëª…í™•í•œ ê°ê´€ì  ì§€í‘œ 2)
3. (ì—°ë„ì™€ ìˆ˜ì¹˜ê°€ ëª…í™•í•œ ê°ê´€ì  ì§€í‘œ 3)
4. (ì—°ë„ì™€ ìˆ˜ì¹˜ê°€ ëª…í™•í•œ ê°ê´€ì  ì§€í‘œ 4)
5. (ì—°ë„ì™€ ìˆ˜ì¹˜ê°€ ëª…í™•í•œ ê°ê´€ì  ì§€í‘œ 5)

ì´ìƒì¹˜:
- í˜ì´ì§€ {{í˜ì´ì§€ë²ˆí˜¸}}: {{ì´ìƒì¹˜ ì§€í‘œëª…}} - {{ì´ìƒì¹˜ ë°œìƒ ì›ì¸ ë° ë³´ê³ ì„œ ë‚´ ê·¼ê±°}}
...

ë³¸ë¬¸:
{pdf_text}
"""
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=1024,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"ERROR: {e}"

def parse_summary_text_with_delta(summary_text):
    """
    GPT ìš”ì•½ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ ìš”ì•½, ì£¼ìš” ì§€í‘œ, ì´ìƒì¹˜ë¡œ ë¶„ë¦¬í•˜ê³ , ì§€í‘œì—ì„œ ì¦ê°ë¥ ì„ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜
    """
    main_summary = []
    detail_summaries = []
    outlier_summaries = []

    lines = summary_text.split('\n')
    current_section = None

    for line in lines:
        line = line.strip()
        if not line:
            continue

        if line.startswith("í•µì‹¬ìš”ì•½:"):
            current_section = "main"
            main_summary = line.replace("í•µì‹¬ìš”ì•½:", "").strip()
        elif line.startswith("ì£¼ìš”ì§€í‘œ:"):
            current_section = "details"
        elif line.startswith("ì´ìƒì¹˜:"):
            current_section = "outliers"
        elif current_section == "details" and line.startswith(('1.', '2.', '3.', '4.', '5.')):
            detail_summaries.append(line)
        elif current_section == "outliers" and line.startswith('-'):
            outlier_summaries.append(line)

    return main_summary, detail_summaries, outlier_summaries

def normalize_period_label(label: str) -> Optional[str]:
    """
    ë‹¤ì–‘í•œ í‘œê¸° â†’ í‘œì¤€:
    - 12/2024A â†’ 2024
    - 12/2025E â†’ 2025F
    - 1Q25E, 1Q2025E â†’ 1Q2025F
    - SEP-243Q â†’ 3Q2024
    - DEC-254QE â†’ 4Q2025F
    - 2026E / 2026F / 2026ENEW â†’ 2026F
    - FY2025E / 2025FY â†’ 2025F, FY2024A â†’ 2024
    - 3Q24 â†’ 3Q2024
    - MAR-25, JUN-25, SEP-25, DEC-25 â†’ 1Q2025 / 2Q2025 / 3Q2025F / 4Q2025F (ê·œì¹™í™”)
    - 2Q2025ACTUAL â†’ 2Q2025
    ì´ë¯¸ í‘œì¤€(YYYY/1QYYYY[F])ì´ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    if label is None:
        return None
    s = str(label).strip().upper().replace(" ", "")

    # íŠ¹ë³„ ì²˜ë¦¬: 2Q2025ACTUAL â†’ 2Q2025
    if s.endswith("ACTUAL"):
        s = s.replace("ACTUAL", "")

    # 1) mm/YYYY + [A/E/F]
    m = re.match(r"^(\d{1,2})/(\d{4})([AEF]?)$", s)
    if m:
        _, yyyy, suf = m.groups()
        yyyy = _to_yyyy(yyyy)
        return f"{yyyy}F" if suf in ("E", "F") else yyyy

    # 2) Quarter: [1-4]QYY(YY)[A/E/F]?
    m = re.match(r"^([1-4])Q(\d{2,4})([AEF]?)$", s)
    if m:
        q, y, suf = m.groups()
        yyyy = _to_yyyy(y)
        return f"{q}Q{yyyy}F" if suf in ("E", "F") else f"{q}Q{yyyy}"

    # 3) MMM-YY + [1-4]Q [A/E/F]? (ë“œë¬¸ ì¼€ì´ìŠ¤)
    m = re.match(r"^([A-Z]{3})-(\d{2})([1-4])Q([AEF]?)$", s)
    if m:
        _, y2, q, suf = m.groups()
        yyyy = _to_yyyy(y2)
        return f"{q}Q{yyyy}F" if suf in ("E", "F") else f"{q}Q{yyyy}"

    # 3.5) MMM-YY ë˜ëŠ” MMM-YYYY â†’ í•´ë‹¹ ë¶„ê¸°(Q)ë¡œ ë³€í™˜ + F ì—¬ë¶€ íŒë‹¨
    m = re.match(r"^([A-Z]{3})[-/](\d{2,4})$", s)
    if m and m.group(1) in _MONTH_TO_Q:
        mon, y = m.groups()
        q = _MONTH_TO_Q[mon]
        yyyy = int(_to_yyyy(y))
        fflag = "F" if _is_future_quarter(yyyy, q) else ""
        return f"{q}Q{yyyy}{fflag}"

    # 4) YYYY(E/F/ENEW..)
    m = re.match(r"^(\d{4})(?:E|F)(?:[A-Z]+)?$", s)
    if m:
        return f"{m.group(1)}F"

    # 5) (FY)?YYYY(FY)?[A/E/F]?
    m = re.match(r"^(?:FY)?(\d{4})(?:FY)?([AEF]?)$", s)
    if m:
        yyyy, suf = m.groups()
        return f"{yyyy}F" if suf in ("E", "F") else yyyy

    # 6) 3Q24 â†’ 3Q2024
    m = re.match(r"^([1-4])Q(\d{2})$", s)
    if m:
        q, y2 = m.groups()
        return f"{q}Q{_to_yyyy(y2)}"

    # ì´ë¯¸ í‘œì¤€ì¼ ìˆ˜ë„ ìˆìŒ
    if re.match(r"^\d{4}F?$", s) or re.match(r"^[1-4]Q\d{4}F?$", s):
        return s

    return s  # ê·œì¹™ ë°–ì´ë©´ ì›ë³¸ ìœ ì§€

def collapse_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ë™ì¼í•œ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì •ê·œí™”ëœ ê²½ìš°, í–‰ë³„ ì²« ìœ íš¨ê°’ìœ¼ë¡œ ë³‘í•©"""
    new_df = pd.DataFrame(index=df.index)
    seen_order = []
    for c in df.columns:
        if c not in seen_order:
            seen_order.append(c)
    for c in seen_order:
        same = [col for col in df.columns if col == c]
        if len(same) == 1:
            new_df[c] = df[c]
        else:
            new_df[c] = df[same].bfill(axis=1).iloc[:, 0]
    return new_df

def normalize_df_columns(df: pd.DataFrame) -> pd.DataFrame:
    """í—¤ë”(ê¸°ê°„) ì •ê·œí™” + Î”/%, consensus, _CHG ì—´ ì œê±° + ì¤‘ë³µ ì»¬ëŸ¼ ë³‘í•©"""
    if df is None or df.empty:
        return df

    # 0) Î”/Delta/%/consensus/_CHG í¬í•¨ ì—´ ë“œë¡­ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    drop_cols = []
    for c in df.columns:
        s = str(c).strip().upper()
        # _CHGë¡œ ëë‚˜ëŠ” ì»¬ëŸ¼ íŠ¹ë³„ ì²´í¬
        if s.endswith("_CHG") or any(tok in s for tok in EXCLUDE_COL_TOKENS):
            drop_cols.append(c)

    if drop_cols:
        df = df.drop(columns=[c for c in drop_cols if c in df.columns], errors="ignore")



    if df.empty:
        return df

    # 1) ê¸°ê°„ ë¼ë²¨ í‘œì¤€í™”
    cols = []
    for c in df.columns:
        norm = normalize_period_label(str(c))
        cols.append(norm if norm is not None else str(c))
    df = df.copy()
    df.columns = cols

    # 2) ì •ê·œí™” í›„ ê°™ì€ ì´ë¦„ ì»¬ëŸ¼ ìƒê¸°ë©´ ë³‘í•©
    if len(set(df.columns)) < len(df.columns):
        df = collapse_duplicate_columns(df)
    return df

def remove_unwanted_rows(df: pd.DataFrame) -> pd.DataFrame:
    """ì›ì¹˜ ì•ŠëŠ” í–‰ íŒ¨í„´ ì œê±° (FCF old, FCF Î”, GP old, GP Î” ë“±)"""
    if df.empty:
        return df

    # ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì†Œë¬¸ìë¡œ ì •ê·œí™”
    idx_lower = df.index.astype(str).str.strip().str.lower()

    # ì œê±°í•  í–‰ë“¤ì„ ì°¾ê¸°
    rows_to_drop = []
    for pattern in EXCLUDE_ROW_PATTERNS:
        mask = idx_lower.str.match(pattern, case=False)
        rows_to_drop.extend(df.index[mask].tolist())

    # ì¤‘ë³µ ì œê±°
    rows_to_drop = list(set(rows_to_drop))

    if rows_to_drop:
        df = df.drop(index=rows_to_drop, errors="ignore")

    return df

# ================== *_mar/jun/sep/dec/fy í–‰ ì ‘ê¸° ==================
_SUFFIX_TO_Q = {"MAR": "1Q", "JUN": "2Q", "SEP": "3Q", "DEC": "4Q"}

def _canon_metric_name(raw_base: str) -> str:
    """ë² ì´ìŠ¤ ì§€í‘œë¥¼ í‘œì¤€ ëª…ì¹­ìœ¼ë¡œ (index_rename_map ì´ìš©), ì‹¤íŒ¨ ì‹œ ì›ë¬¸ íŠ¸ë¦¼"""
    k = raw_base.strip().lower()
    return index_rename_map.get(k, raw_base.strip())

def _is_year_col(col: str) -> Optional[tuple[str, bool]]:
    """ì—°ë„ ì»¬ëŸ¼ì¸ì§€ í™•ì¸. return (YYYY, is_forecast)"""
    m = re.match(r"^(\d{4})(F?)$", str(col))
    if not m:
        return None
    yyyy, f = m.groups()
    return yyyy, (f == "F")

def fold_month_suffix_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    revenue_mar/jun/sep/dec/fy â†’ ë² ì´ìŠ¤ ì§€í‘œë¡œ ì ‘ê¸°
    *_mar â†’ 1QYYYY, *_jun â†’ 2QYYYY, *_sep â†’ 3QYYYY, *_dec â†’ 4QYYYY
    *_fy, *_fy rm â†’ YYYY / YYYYF (ì—°ë„ ì»¬ëŸ¼ í˜•íƒœ ìœ ì§€)
    - ì›ë³¸ suffix í–‰ì€ ì œê±°, ë² ì´ìŠ¤ í–‰ì´ ì—†ìœ¼ë©´ ìƒì„±
    - ê¸°ì¡´ê°’ì´ NaNì´ë©´ ì±„ì›€, ê°’ì´ ìˆìœ¼ë©´ ë³´ì¡´
    """
    if df.empty:
        return df

    df = df.copy()
    idx_series = pd.Index([str(i) for i in df.index])
    work_rows = []

    # ì–¸ë”ìŠ¤ì½”ì–´/í•˜ì´í”ˆ/ìŠ¬ë˜ì‹œ/ê³µë°± êµ¬ë¶„ì ëª¨ë‘ í—ˆìš© + FY RM í—ˆìš©
    pattern = re.compile(r"^(.*?)(?:\s*\(.*?\))?[\s_\-\/]+(MAR|JUN|SEP|DEC|FY(?:\s*RM)?)$",
                         flags=re.IGNORECASE)
    for i in idx_series:
        m = pattern.match(i.strip())
        if m:
            base_raw, suf = m.groups()
            suf = suf.upper().replace(" ", "")
            if suf.startswith("FY"):
                suf = "FY"
            work_rows.append((i, base_raw, suf))

    if not work_rows:
        return df

    for original_name, base_raw, suf in work_rows:
        base_std = _canon_metric_name(base_raw)
        row = df.loc[original_name]

        for col in df.columns:
            yinfo = _is_year_col(col)
            if not yinfo:
                continue
            yyyy, isF = yinfo

            if suf == "FY":
                target_col = f"{yyyy}F" if isF else yyyy
            else:
                q = _SUFFIX_TO_Q[suf]
                target_col = f"{q}{yyyy}F" if isF else f"{q}{yyyy}"

            val = row.get(col)
            if pd.isna(val):
                continue

            if base_std not in df.index:
                df.loc[base_std, :] = pd.NA
            if target_col not in df.columns:
                df[target_col] = pd.NA

            if pd.isna(df.at[base_std, target_col]):
                df.at[base_std, target_col] = val

        # ì›ë³¸ suffix í–‰ ì œê±° (ì˜ˆ: revenue-jun, revenue-dec ë“± í‘œê¸° ì•ˆë˜ê²Œ)
        df = df.drop(index=original_name, errors="ignore")
    return df

def drop_unwanted_revenue_rows(df: pd.DataFrame) -> pd.DataFrame:
    """
    ìš”ì²­: revenue-Net, revenue-Fy Rm, revenue-Fy, revenue-Dec ëŠ” í‘œê¸° ì•ˆë˜ê²Œ ì œê±°
    (fold ì´í›„ ì”ì¡´ ì‹œ ì•ˆì „í•˜ê²Œ í•„í„°ë§)
    """
    if df.empty:
        return df
    idx = df.index.astype(str).str.strip().str.lower()
    pattern = r"^revenue[\s_\-\/]+(net|fy(?:rm)?|dec)$"
    mask = idx.str.match(pattern)
    return df.loc[~mask].copy()

def handle_actual_columns(df: pd.DataFrame) -> pd.DataFrame:
    """2Q2025ACTUAL ê°™ì€ ì»¬ëŸ¼ì„ ì²˜ë¦¬: ë°ì´í„°ë¥¼ 2Q25ë¡œ ì´ë™í•˜ê³  ì›ë³¸ ì»¬ëŸ¼ ì œê±°"""
    if df.empty:
        return df

    df = df.copy()
    cols_to_process = []

    # ACTUALì´ í¬í•¨ëœ ì»¬ëŸ¼ ì°¾ê¸°
    for col in df.columns:
        col_str = str(col).upper()
        if "ACTUAL" in col_str:
            # ACTUAL ì œê±°í•œ ë²„ì „ ìƒì„±
            base_col = col_str.replace("ACTUAL", "")
            # ì •ê·œí™” ì ìš©
            normalized_col = normalize_period_label(base_col)
            if normalized_col:
                cols_to_process.append((col, normalized_col))

    # ë°ì´í„° ì´ë™ ë° ì»¬ëŸ¼ ì œê±°
    for original_col, target_col in cols_to_process:
        if target_col not in df.columns:
            df[target_col] = pd.NA

        # ë°ì´í„° ì´ë™ (ê¸°ì¡´ ê°’ì´ NaNì¸ ê²½ìš°ë§Œ)
        for idx in df.index:
            if pd.isna(df.at[idx, target_col]) and pd.notna(df.at[idx, original_col]):
                df.at[idx, target_col] = df.at[idx, original_col]

        # ì›ë³¸ ì»¬ëŸ¼ ì œê±°
        df = df.drop(columns=[original_col], errors="ignore")

    return df

# ================== Revenue ì„¸ê·¸ë¨¼íŠ¸(íšŒì‚¬ ë¬´ê´€) ìë™ ê°ì§€ ==================
SKIP_SEGMENT_WORDS = ("growth", "margin", "qoq", "yoy", "mix", "asp", "price", "chg", "change")

def extract_revenue_segments_generic(df: pd.DataFrame) -> pd.DataFrame:
    """
    'revenue_xxx', 'revenue-xxx', 'xxx_revenue', 'xxx revenue' â†’ Revenue-TitleCase
    growth/margin/qoq/yoy ë“± ì§€í‘œì„± ë‹¨ì–´ëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¡œ ë³´ì§€ ì•ŠìŒ.
    """
    outs = []
    for idx in pd.Index(df.index.astype(str)).unique():
        s = idx.strip()
        low = s.lower()
        if "revenue" not in low:
            continue

        m = (re.match(r"^revenue[\s\-_\/]+(.+)$", low)
             or re.match(r"^(.+)[\s\-_\/]+revenue$", low))
        if not m:
            continue

        seg = m.group(1)
        seg = re.sub(r"\(.*?\)", "", seg).strip()
        if not seg or any(w in seg for w in SKIP_SEGMENT_WORDS):
            continue

        # ì›ì¹˜ ì•ŠëŠ” ì„¸ê·¸ë¨¼íŠ¸( net / fy / fy rm / dec )ëŠ” ì œì™¸
        if re.match(r"^(net|fy(?:\s*rm)?|dec)$", seg):
            continue

        seg_title = re.sub(r"[_\-\/]+", " ", seg).title()
        row = df.loc[df.index.astype(str) == idx].copy()
        row.index = [f"revenue-{seg_title}"] * len(row)
        outs.append(row)

    return pd.concat(outs) if outs else pd.DataFrame()

# ================== AMD í…œí”Œë¦¿ ë°ì´í„° ìƒì„± ==================
def create_amd_template_df() -> pd.DataFrame:
    columns = ['Company', '2023', '2024', '2025F', '2026F', '2027F',
               '1Q24', '2Q24', '3Q24', '4Q24', '1Q25', '2Q25', '3Q25F', '4Q25F']
    amd_df = pd.DataFrame(columns=columns)

    for metric, data in amd_template_data.items():
        row_data = ['AMD']
        row_data.extend([
            data.get(2023, ''), data.get(2024, ''), data.get('2025F', ''),
            data.get('2026F', ''), ''
        ])
        row_data.extend([
            data.get('1Q24', ''), data.get('2Q24', ''), data.get('3Q24', ''), data.get('4Q24', ''),
            data.get('1Q25', ''), data.get('2Q25', ''), data.get('3Q25F', ''), data.get('4Q25F', '')
        ])
        amd_df.loc[metric] = row_data

    if 'OP' in amd_df.index and 'Revenue' in amd_df.index:
        op_margin_row = ['AMD']
        for col in ['2023', '2024', '2025F', '2026F']:
            try:
                op_val = float(amd_df.loc['OP', col]) if amd_df.loc['OP', col] != '' else None
                rev_val = float(amd_df.loc['Revenue', col]) if amd_df.loc['Revenue', col] != '' else None
                op_margin_row.append(round((op_val / rev_val) * 100, 1) if (op_val is not None and rev_val) else '')
            except:
                op_margin_row.append('')
        op_margin_row.append('')  # 2027F

        for col in ['1Q24', '2Q24', '3Q24', '4Q24', '1Q25', '2Q25', '3Q25F', '4Q25F']:
            try:
                op_val = float(amd_df.loc['OP', col]) if amd_df.loc['OP', col] != '' else None
                rev_val = float(amd_df.loc['Revenue', col]) if amd_df.loc['Revenue', col] != '' else None
                op_margin_row.append(round((op_val / rev_val) * 100, 1) if (op_val is not None and rev_val) else '')
            except:
                op_margin_row.append('')
        amd_df.loc['OP margin'] = op_margin_row

    return amd_df

# ================== AMD ê¸°ì—… ê°ì§€ ë° í…œí”Œë¦¿ ì ìš© ==================
def is_amd_company(company_name: str) -> bool:
    if not company_name:
        return False
    name_lower = company_name.lower()
    return any(keyword in name_lower for keyword in ['amd', 'advanced micro devices'])

def apply_amd_template_if_needed(df: pd.DataFrame, company_name: str) -> pd.DataFrame:
    if not is_amd_company(company_name):
        return df

    template_df = create_amd_template_df()
    if df is None or df.empty:
        return template_df

    combined_df = df.copy()
    for metric in template_df.index:
        if metric not in combined_df.index:
            combined_df.loc[metric] = template_df.loc[metric]
        else:
            for col in template_df.columns:
                if col in combined_df.columns:
                    existing_val = combined_df.loc[metric, col]
                    template_val = template_df.loc[metric, col]
                    if (pd.isna(existing_val) or existing_val == '') and template_val != '':
                        combined_df.loc[metric, col] = template_val
    return combined_df

# ================== í‘œì‹œìš© ê°„ëµ ë¼ë²¨(ì˜ˆì¸¡ì¹˜ F ìœ ì§€) ==================
def _compact_period_label(s: str, keep_F: bool = True) -> str:
    s = str(s)
    m = re.match(r"^([1-4])Q(20)?(\d{2})(F?)$", s)
    if m:
        q, _20, yy, f = m.groups()
        out = f"{q}Q{yy}"
        if keep_F and f == "F":
            out += "F"
        return out

    m = re.match(r"^(\d{4})(F?)$", s)
    if m:
        yyyy, f = m.groups()
        out = yyyy
        if keep_F and f == "F":
            out += "F"
        return out
    return s

def to_compact_columns(df: pd.DataFrame, keep_F: bool = True) -> pd.DataFrame:
    if df is None or df.empty:
        return df
    new = df.copy()
    new.columns = [_compact_period_label(c, keep_F=keep_F) for c in new.columns]
    if len(set(new.columns)) < len(new.columns):
        new = collapse_duplicate_columns(new)
    return new

# ================== PDF â†’ í…ìŠ¤íŠ¸ ==================
def extract_text_from_pdf(file) -> list[str]:
    doc = fitz.open(stream=file.read(), filetype="pdf")
    return [page.get_text() for page in doc]

# ================== GPT í‘œ ì¶”ì¶œ ==================
def extract_tables_with_gpt(text: str) -> str:
    prompt = f"""
ë‹¤ìŒ ì´ë¯¸ì§€ëŠ” PDF ë³´ê³ ì„œì˜ í•œ í˜ì´ì§€ì…ë‹ˆë‹¤. ì´ í˜ì´ì§€ ì•ˆì— ìˆëŠ” ëª¨ë“  í‘œë¥¼ CSV í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
- CSVëŠ” í—¤ë” í¬í•¨, ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ ì…€ êµ¬ë¶„í•©ë‹ˆë‹¤.

ì¡°ê±´:
1. í‘œê°€ í•˜ë‚˜ë„ ì—†ë‹¤ë©´, "NONE"ì´ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.
2. í‘œëŠ” ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‘œê°€ ì—¬ëŸ¬ ê°œë©´ ê°œë³„ì ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
   ë‹¨, í‘œ ì‚¬ì´ì— **ì—°ë„ í˜•ì‹(ì˜ˆ: 2022, 1Q25)ì´ ì—†ìœ¼ë©´** ê°™ì€ í‘œë¡œ ê°„ì£¼í•˜ì„¸ìš”.
3. í‘œ ì¤‘ê°„ì— ë¹ˆ ì…€(ê°’ì´ ì—†ëŠ” ê³µê°„)ì€ ë°˜ë“œì‹œ "NaN"ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.
4. ìˆ«ì(ì˜ˆ: 123, 45.67)ì™€ ì—°ë„(ì˜ˆ: 2022, 1Q25)ëŠ” ì •í™•íˆ ì¸ì‹í•´ì£¼ì„¸ìš”.
5. í˜ì´ì§€ ë‚´ í…ìŠ¤íŠ¸ ì¤‘ ì‰¼í‘œ(,)ëŠ” ì…€ êµ¬ë¶„ìê°€ ì•„ë‹™ë‹ˆë‹¤.
   ë§Œì•½ ì‰¼í‘œê°€ ì…€ ì•ˆ í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì‚­ì œí•´ì£¼ì„¸ìš”.
6. í‘œ êµ¬ì„± ì‹œ ë°˜ë“œì‹œ ë‹¤ìŒ ê¸°ì¤€ì„ ë”°ë¥´ì„¸ìš”:
   6-1. í‘œ í—¤ë”ëŠ” ë¬´ì¡°ê±´ ì—°ë„/ë¶„ê¸° í˜•ì‹(ì˜ˆ: 2022, 1Q25 ë“±)ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”.
   6-2. ì²« ë²ˆì§¸ ì—´ì˜ í—¤ë”ëŠ” í•­ìƒ "index"ë¡œ ì§€ì •í•˜ì„¸ìš”.
   6-3. ì§€í‘œëª…ì€ í•­ìƒ index ì—´ì— ìœ„ì¹˜ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
   6-5. **í—¤ë”ì— ë…„ë„/ë¶„ê¸° í˜•ì‹ì´ í¬í•¨ë˜ì–´ìˆì§€ ì•Šì€ í‘œ**ëŠ” ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
   6-6. ê´„í˜¸ ì•ˆ ë‹¨ìœ„ëŠ” indexì— ê°™ì´ í‘œê¸°, headerì—ëŠ” ê´„í˜¸ ê¸ˆì§€.
   8. ë³‘ë ¬ í‘œëŠ” ë¶„ë¦¬ ì¶”ì¶œ.
   9. "TTB"ëŠ” "í‘ì „"ìœ¼ë¡œ ë³€ê²½.
   10. indexë§Œ ìˆê³  ë‚˜ë¨¸ì§€ NONEì¸ í–‰ë„ ìœ ì§€.
   11. ìƒí•˜ìœ„ ì§€í‘œ ê´€ê³„ëŠ” ìƒìœ„_í•˜ìœ„ í˜•íƒœë¡œ ì¡°ì • (Revenue_DRAM ë“±).
   12. AMD ê´€ë ¨ íŠ¹ìˆ˜ ì²˜ë¦¬:
       - "Data Center", "Client", "Gaming", "Embedded" ë“±ì€ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì¸ì‹
       - "Net Revenue", "Cost of Sales", "Gross Profit", "Operating Income" ë“± AMD ìš©ì–´ë„ ì¶”ì¶œ
       - "Non-GAAP EPS"ëŠ” "EPS"ë¡œ ì²˜ë¦¬

ì¶œë ¥ ì˜ˆì‹œ:
index;2022;2022ì¶”ì •;1Q25
FCF;1000;1100;1200
Revenue;5000;5200;5400

í…ìŠ¤íŠ¸:
{text}
"""
    try:
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=4096,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        return f"ERROR: {e}"

def process_single_page(i: int, page_text: str):
    out = extract_tables_with_gpt(page_text)
    return (i, out)

# ================== ì „ì²˜ë¦¬/ë³‘í•© ìœ í‹¸ ==================
def make_index_unique(index_list):
    count = defaultdict(int)
    unique = []
    for name in index_list:
        if count[name] == 0:
            unique.append(name)
        else:
            unique.append(f"{name} ({count[name]})")
        count[name] += 1
    return unique

def match_and_rename_index(idx: str) -> Optional[str]:
    idx = str(idx).strip().lower()
    pattern = r"^([a-z\s]+?)(\s*\(.*?\))?$"
    m = re.match(pattern, idx)
    if not m or m.group(0) != idx:
        return None
    base, suffix = m.groups()
    suffix = suffix or ""
    renamed = index_rename_map.get(base.strip())
    return f"{renamed}{suffix}" if renamed else None

def group_name_match(name: str) -> Optional[str]:
    name = str(name).lower()
    for group, keywords in {
        "CapEx": ["capital expenditures", "capital expenditure", "capex", "purchase of property", "purchases of property", "purchase of pp&e", "additions to property", "acquisition of property", "investment in property"],
        "Acquisition & Equity Investment": ["acquisition", "business combinations", "purchase of subsidiaries", "investment in associates", "investment in affiliates", "equity investment", "purchase of business"],
        "Intangible asset": ["intangible assets", "purchase of intangible", "software development", "internal-use software", "capitalized development costs", "goodwill and intangibles"],
        "others": ["investing activities", "purchase of securities", "marketable securities", "financial investment", "long-term investment"],
    }.items():
        if any(k == name for k in keywords):
            return group
    return None

def get_base_name(idx: str) -> str:
    name = str(idx).strip()
    name = re.sub(r"\s*\([^\)]*\)", "", name)
    return name.strip()

def merge_duplicate_rows(df: pd.DataFrame, tolerance=0.05, large_diff_target=1000, tolerance_ratio=0.05):
    merged_rows = []
    unmerged_rows = []

    df_copy = df.copy()
    df_copy.index = df_copy.index.astype(str)
    grouped = df_copy.groupby(get_base_name)

    for base_idx, group in grouped:
        if len(group) == 1:
            row = group.iloc[0].copy()
            row.name = base_idx
            merged_rows.append(row)
            continue

        merged = group.iloc[0].copy()
        for i in range(1, len(group)):
            current = group.iloc[i]
            this_merge_possible = True
            scale_entire_row = False

            for col in merged.index:
                val1 = merged[col]
                val2 = current[col]

                if pd.isna(val1) and pd.isna(val2):
                    continue
                if pd.isna(val1) or pd.isna(val2):
                    continue

                try:
                    val1 = float(val1) if pd.notna(val1) else float('nan')
                    val2 = float(val2) if pd.notna(val2) else float('nan')
                except (ValueError, TypeError):
                    this_merge_possible = False
                    break

                max_val = max(abs(val1), abs(val2))
                min_val = min(abs(val1), abs(val2)) + 1e-12
                diff = abs(val1 - val2)
                rel_diff = diff / (max_val if max_val != 0 else 1)

                if rel_diff <= tolerance:
                    continue

                ratio = max_val / min_val
                lower = large_diff_target * (1 - tolerance_ratio)
                upper = large_diff_target * (1 + tolerance_ratio)
                if lower <= ratio <= upper:
                    scale_entire_row = True
                else:
                    this_merge_possible = False
                    break

            if not this_merge_possible:
                unmerged_rows.append(current.copy())
                continue

            if scale_entire_row:
                try:
                    merged_numeric = merged.apply(pd.to_numeric, errors='coerce')
                    current_numeric = current.apply(pd.to_numeric, errors='coerce')
                    total1 = merged_numeric.sum(skipna=True)
                    total2 = current_numeric.sum(skipna=True)

                    if total1 < total2:
                        scaled = merged_numeric * large_diff_target
                        base_row = current_numeric.copy()
                    else:
                        scaled = current_numeric * large_diff_target
                        base_row = merged_numeric.copy()

                    result = base_row.copy()
                    for col in result.index:
                        if pd.isna(result[col]) and not pd.isna(scaled[col]):
                            result[col] = scaled[col]
                    merged = result
                except Exception as e:
                    st.warning(f"ìŠ¤ì¼€ì¼ë§ ì¤‘ ê²½ê³ : {e}. ì´ í–‰ì€ ìŠ¤ì¼€ì¼ë§ ì—†ì´ ë³‘í•©ì„ ì‹œë„í•©ë‹ˆë‹¤.")
                    for col in merged.index:
                        if pd.isna(merged[col]) and pd.notna(current[col]):
                            try:
                                merged[col] = float(current[col])
                            except Exception:
                                pass
            else:
                for col in merged.index:
                    if pd.isna(merged[col]) and pd.notna(current[col]):
                        try:
                            merged[col] = float(current[col])
                        except Exception:
                            pass

        merged.name = base_idx
        merged_rows.append(merged)

    df_merged = pd.DataFrame(merged_rows)

    if unmerged_rows:
        df_unmerged = pd.DataFrame(unmerged_rows)
        df_unmerged.index = df_unmerged.index.astype(str)
        df_unmerged.index = make_index_unique(df_unmerged.index.tolist())
        df_merged = pd.concat([df_merged, df_unmerged], axis=0)

    df_merged.index.name = None
    return df_merged

# ================== DF ì„¸íŠ¸ ì²˜ë¦¬ ==================
def process_extracted_dfs(list_of_dfs: list[pd.DataFrame], company_name: Optional[str]):
    errors = []
    if not list_of_dfs:
        return None, ["ìœ íš¨í•œ DataFrameì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."], None

    # (A) ê° DF ì‚¬ì „ ì •ê·œí™”: í—¤ë” ì •ê·œí™” + suffix í–‰ ì ‘ê¸° + ì›ì¹˜ ì•ŠëŠ” revenue í–‰ ì œê±° + ACTUAL ì»¬ëŸ¼ ì²˜ë¦¬ + ì›ì¹˜ ì•ŠëŠ” í–‰ ì œê±°
    cleaned = []
    for df in list_of_dfs:
        df = normalize_df_columns(df)
        df = handle_actual_columns(df)  # ìƒˆë¡œ ì¶”ê°€: ACTUAL ì»¬ëŸ¼ ì²˜ë¦¬
        df = fold_month_suffix_rows(df)
        df = drop_unwanted_revenue_rows(df)
        df = remove_unwanted_rows(df)  # ìƒˆë¡œ ì¶”ê°€: FCF old, GP old ë“± ì œê±°
        cleaned.append(df)

    df_merged = pd.concat(cleaned, axis=0)
    df_merged.index = df_merged.index.astype(str).str.strip().str.lower()

    # 1) ì§€í‘œëª… ë§¤í•‘
    matched_rows = []
    for idx in df_merged.index.unique():
        new_idx = match_and_rename_index(idx)
        if new_idx:
            row = df_merged.loc[[idx]]
            row.index = [new_idx] * len(row)
            matched_rows.append(row)
    df_index_map = pd.concat(matched_rows) if matched_rows else pd.DataFrame()
    if not df_index_map.empty:
        df_index_map.index = make_index_unique(df_index_map.index.tolist())

    # 2) í‚¤ì›Œë“œ ê·¸ë£¹ ë§¤í•‘
    group_rows = []
    for idx in df_merged.index:
        group = group_name_match(idx)
        if group:
            row = df_merged.loc[[idx]]
            row.index = [group] * len(row)
            group_rows.append(row)
    df_group_kw = pd.concat(group_rows) if group_rows else pd.DataFrame()
    if not df_group_kw.empty:
        df_group_kw.index = make_index_unique(df_group_kw.index.tolist())

    # 3) ì‚¬ì—…ë¶€ë¬¸ ë§¤í•‘ (ì‚¬ì „ + generic)
    df_segment = pd.DataFrame()

    if company_name and company_name in company_segments:
        for seg in company_segments[company_name]:
            seg_lower = seg.lower()
            matched = df_merged[
                df_merged.index.astype(str).str.contains("revenue")
                & df_merged.index.astype(str).str.contains(seg_lower)
            ]
            if not matched.empty:
                matched = matched.copy()
                matched.index = [f"revenue-{seg}"] * len(matched)
                df_segment = pd.concat([df_segment, matched])

        other_match = df_merged[
            df_merged.index.astype(str).str.contains("revenue")
            & df_merged.index.astype(str).str.contains("other")
        ]
        if not other_match.empty:
            other_match = other_match.copy()
            other_match.index = ["revenue-other"] * len(other_match)
            df_segment = pd.concat([df_segment, other_match])

    df_segment_generic = extract_revenue_segments_generic(df_merged)
    if not df_segment_generic.empty:
        df_segment = pd.concat([df_segment, df_segment_generic], axis=0)

    if not df_segment.empty:
        df_segment.index = make_index_unique(df_segment.index.tolist())

    # 4) í†µí•© í›„ ì¤‘ë³µ ë³‘í•©
    final_result = pd.concat([df_index_map, df_group_kw, df_segment], axis=0)
    final_result.index = final_result.index.astype(str)

    # 4.5 revenue-ì„¸ê·¸ë¨¼íŠ¸ ì¤‘ë³µ ì œê±°(ì²« í•­ëª© ìš°ì„ )
    revenue_rows = final_result[final_result.index.str.startswith("revenue-")]
    non_revenue_rows = final_result[~final_result.index.str.startswith("revenue-")]
    seen = set()
    deduped_revenue_rows = []
    for idx in revenue_rows.index:
        base = re.sub(r"\s*\(\d+\)$", "", str(idx))
        if base not in seen:
            seen.add(base)
            deduped_revenue_rows.append(revenue_rows.loc[idx])
    df_revenue_unique = pd.DataFrame(deduped_revenue_rows)
    final_result = pd.concat([non_revenue_rows, df_revenue_unique], axis=0)

    # 5) ì¤‘ë³µ ì¸ë±ìŠ¤ ë³‘í•©
    final_result.index = final_result.index.astype(str)
    final_result_unique = merge_duplicate_rows(
        final_result, tolerance=0.05, large_diff_target=1000, tolerance_ratio=0.05
    )

    # 5-1) ( ... ) í¬í•¨ ì¸ë±ìŠ¤ ì œê±°
    final_result_unique.index = final_result_unique.index.astype(str)
    final_result_unique = final_result_unique[~final_result_unique.index.str.contains(r"\(.*?\)")]

    # 6) AMDì¸ ê²½ìš° í…œí”Œë¦¿ ì ìš©
    if company_name:
        final_result_unique = apply_amd_template_if_needed(final_result_unique, company_name)

    # ëˆ„ë½ ì„¸ê·¸ë¨¼íŠ¸ ë³´ê³  (ì‚¬ì „ ê¸°ë°˜ì¸ ê²½ìš°ë§Œ)
    missing_segments = None
    if company_name and company_name in company_segments:
        expected = set(company_segments[company_name])
        actual = set(
            idx.replace("revenue-", "").split(" ")[0]
            for idx in final_result_unique.index.astype(str)
            if idx.startswith("revenue-")
        )
        missing = expected - actual
        if missing:
            missing_segments = list(missing)

    return final_result_unique, errors, missing_segments

# ================== ì‹œê°í™” ê´€ë ¨ í•¨ìˆ˜ë“¤ ==================
def normalize_period(x: str) -> str:
    x = str(x).strip().upper().replace(" ", "")
    m = re.match(r"^([1-4])Q(\d{2,4})F?$", x)      # 1Q25, 2Q2025, 3Q25F...
    if m:
        q, y = m.groups()
        if len(y) == 2: y = "20" + y
        return f"{q}Q{y}F"  # ë¶„ê¸°ëŠ” F ìœ ë¬´ ì„ì—¬ë„ Fë¡œ í†µì¼
    m = re.match(r"^(\d{2,4})F?$", x)              # 2024, 25F ...
    if m:
        y = m.group(1)
        if len(y) == 2: y = "20" + y
        return f"{y}F" if "F" in x else y
    return x

def year_sort_key(s):
    s = re.sub(r"\D", "", str(s).replace("F", ""))
    if len(s) == 2: s = "20" + s
    return int(s) if s else 0

def quarter_sort_key(s):
    m = re.match(r"^([1-4])Q(\d{2,4})F?$", str(s).upper())
    if m:
        q, y = m.groups()
        if len(y) == 2: y = "20" + y
        return int(y) * 10 + int(q)
    return 0

def read_flexible_csv(uploaded_file) -> pd.DataFrame:
    raw = uploaded_file.read()
    sample = raw[:4096].decode("utf-8", errors="ignore")
    sep = ";" if sample.count(";") > sample.count(",") else ","
    return pd.read_csv(io.BytesIO(raw), sep=sep, engine="python")

def is_period_col(name: str) -> bool:
    s = re.sub(r"\s+", "", str(name)).upper()
    return bool(re.fullmatch(r"\d{2,4}F?", s) or re.fullmatch(r"[1-4]Q\d{2,4}F?", s))

def tidy_long(df: pd.DataFrame) -> pd.DataFrame:
    """DataFrameì„ ì„¸ë¡œí˜•ìœ¼ë¡œ ë³€í™˜. ì»¬ëŸ¼ êµ¬ì¡°ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì•ˆì „í•˜ê²Œ ì²˜ë¦¬"""
    if df is None or df.empty:
        return pd.DataFrame(columns=["company", "segment", "ì‹œì ", "value"])

    df = df.copy()

    # ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
    new_columns = []
    seen = {}
    for col in df.columns:
        col_str = str(col).strip()
        if col_str in seen:
            seen[col_str] += 1
            new_columns.append(f"{col_str}_{seen[col_str]}")
        else:
            seen[col_str] = 0
            new_columns.append(col_str)

    df.columns = new_columns

    lower_map = {c.lower(): c for c in df.columns}

    # Company ì»¬ëŸ¼ ì°¾ê¸°
    company_col = None
    for cand in ["company", "ê¸°ì—…", "íšŒì‚¬", "brand", "maker"]:
        if cand in lower_map:
            company_col = lower_map[cand]
            break

    if not company_col:
        st.error("Company ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSVì— 'Company' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame(columns=["company", "segment", "ì‹œì ", "value"])

    # Segment ì»¬ëŸ¼ ì°¾ê¸° - ë” ì ê·¹ì ìœ¼ë¡œ!
    segment_col = None

    # 1) ëª…ì‹œì  segment ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    for cand in ["segment", "metric", "ì§€í‘œ", "í•­ëª©", "indicator", "ê³„ì •", "ê³„ì •ê³¼ëª©"]:
        if cand in lower_map and lower_map[cand] != company_col:
            segment_col = lower_map[cand]
            break

    # 2) ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë¬´ëª…ì´ë©´ ê°•ì œë¡œ ì‚¬ìš©
    if not segment_col:
        first_col = df.columns[0]
        if first_col != company_col:  # Company ì»¬ëŸ¼ì´ ì•„ë‹ˆë©´
            segment_col = first_col

    # 3) ì—¬ì „íˆ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©
    if not segment_col:
        if df.index.name and df.index.name != company_col:
            df = df.reset_index()
            segment_col = df.columns[0]  # ì¸ë±ìŠ¤ê°€ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë¨

        else:
            # ì¸ë±ìŠ¤ë¥¼ ê°•ì œë¡œ ì»¬ëŸ¼ìœ¼ë¡œ ë§Œë“¤ê¸°
            df = df.reset_index()
            df = df.rename(columns={'index': 'segment_index'})
            segment_col = 'segment_index'


    # 4) ê·¸ë˜ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìƒì„±
    if not segment_col or segment_col not in df.columns:
        df.insert(0, 'segment_default', df.index.astype(str))
        segment_col = 'segment_default'

    # ì´ë¯¸ ì„¸ë¡œí˜•ì¸ì§€ í™•ì¸
    if any(c in lower_map for c in ["ì‹œì ", "period", "ê¸°ê°„", "value"]):
        period_col = None
        value_col = None

        for cand in ["ì‹œì ", "period", "ê¸°ê°„"]:
            if cand in lower_map:
                period_col = lower_map[cand]
                break

        for cand in ["value", "ê°’", "amount"]:
            if cand in lower_map:
                value_col = lower_map[cand]
                break

        if period_col and value_col:
            out = df.rename(columns={
                company_col: "company",
                segment_col: "segment",
                period_col: "ì‹œì ",
                value_col: "value"
            }).copy()
            out["value"] = pd.to_numeric(out["value"], errors="coerce")
            out["ì‹œì "] = out["ì‹œì "].astype(str).apply(normalize_period)
            return out[["company", "segment", "ì‹œì ", "value"]]

    # ê°€ë¡œí˜• â†’ ì„¸ë¡œí˜• ë³€í™˜
    # ì‹œì  ì»¬ëŸ¼ ì°¾ê¸° (company, segment ì œì™¸)
    period_cols = []
    for c in df.columns:
        if c != company_col and c != segment_col and is_period_col(c):
            period_cols.append(c)

    # ì¤‘ë³µ ì œê±°
    period_cols = list(dict.fromkeys(period_cols))

    if not period_cols:
        st.error(f"ì‹œì ìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        st.write(f"**ì œì™¸ëœ ì»¬ëŸ¼**: Company={company_col}, Segment={segment_col}")

        # ëª¨ë“  ì»¬ëŸ¼ì„ ì‹œì  ì»¬ëŸ¼ í›„ë³´ë¡œ ì²´í¬í•´ë³´ê¸°
        candidates = []
        for c in df.columns:
            if c not in [company_col, segment_col]:
                candidates.append(f"{c} (is_period: {is_period_col(c)})")

        st.write(f"**ì‹œì  ì»¬ëŸ¼ í›„ë³´ë“¤**: {candidates}")
        st.error("ì—°ë„(ì˜ˆ: 2024, 2025F) ë˜ëŠ” ë¶„ê¸°(ì˜ˆ: 1Q24, 2Q25F) í˜•ì‹ì˜ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return pd.DataFrame(columns=["company", "segment", "ì‹œì ", "value"])


    # í•„ìš”í•œ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒ
    use_cols = [segment_col, company_col] + period_cols
    use_cols = [c for c in use_cols if c in df.columns]

    if len(use_cols) < 3:
        st.error(f"ë³€í™˜ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {use_cols}")
        return pd.DataFrame(columns=["company", "segment", "ì‹œì ", "value"])

    tmp = df[use_cols].copy()

    # ì»¬ëŸ¼ëª… ì •ê·œí™”
    tmp = tmp.rename(columns={
        company_col: "company",
        segment_col: "segment"
    })


    period_cols_final = [c for c in period_cols if c in tmp.columns]

    # melt ì‹¤í–‰
    try:
        long = tmp.melt(
            id_vars=["segment", "company"],
            value_vars=period_cols_final,
            var_name="ì‹œì ",
            value_name="value"
        )
    except Exception as e:
        st.error(f"ë°ì´í„° ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        st.write("**ë””ë²„ê·¸ ì •ë³´:**")
        st.write(f"- tmp.shape: {tmp.shape}")
        st.write(f"- tmp.columns: {list(tmp.columns)}")
        st.write(f"- period_cols_final: {period_cols_final}")
        return pd.DataFrame(columns=["company", "segment", "ì‹œì ", "value"])

    # ê°’ ì •ë¦¬ ë° ë³€í™˜
    long["value"] = (
        long["value"].astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("%", "", regex=False)
        .str.replace(r"^[â€“â€”-]+$", "", regex=True)
        .str.replace("NaN", "", regex=False)
        .str.replace("nan", "", regex=False)
        .str.strip()
    )

    # ë¹ˆ ê°’ë“¤ì„ NaNìœ¼ë¡œ ë³€í™˜
    long["value"] = long["value"].replace("", pd.NA)
    long["value"] = pd.to_numeric(long["value"], errors="coerce")

    # ì‹œì  ì •ê·œí™”
    long["ì‹œì "] = long["ì‹œì "].astype(str).apply(normalize_period)

    # ìœ íš¨í•œ ë°ì´í„°ë§Œ ë°˜í™˜
    long = long.dropna(subset=["ì‹œì "])

    # ë¹ˆ segmentë‚˜ company ì œê±°
    long = long.dropna(subset=["company", "segment"])
    long = long[long["segment"].astype(str).str.strip() != ""]
    long = long[long["company"].astype(str).str.strip() != ""]

    # ìµœì¢… ê²°ê³¼ ì •ë³´
    if len(long) > 0:
        st.success(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(long)}ê°œ í–‰, {long['company'].nunique()}ê°œ ê¸°ì—…, {long['segment'].nunique()}ê°œ ì§€í‘œ")
    else:
        st.warning("âš ï¸ ë³€í™˜ì€ ì„±ê³µí–ˆì§€ë§Œ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.write("**ë³€í™˜ëœ ë°ì´í„° í™•ì¸:**")
        st.write(long.head() if not long.empty else "ë¹ˆ DataFrame")

    return long[["company", "segment", "ì‹œì ", "value"]]

# ================== FY/CY ì°¸ê³  ì •ë³´ ==================
FY_CY_INFO = [
    {"match": r"(?i)ì—”ë¹„ë””ì•„|nvidia", "fy_end": "1ì›” ë§(ì£¼ ë‹¨ìœ„ ì¢…ê²°)", "cy_aligned": False, "extra": "FY=2~1ì›”"},
    {"match": r"(?i)ì•„ë§ˆì¡´|amazon", "fy_end": "12ì›” 31ì¼", "cy_aligned": True, "extra": "CY=FY"},
    {"match": r"(?i)ì•ŒíŒŒë²³|êµ¬ê¸€|alphabet|google", "fy_end": "12ì›” 31ì¼", "cy_aligned": True, "extra": "CY=FY"},
    {"match": r"(?i)ë©”íƒ€|meta", "fy_end": "12ì›” 31ì¼", "cy_aligned": True, "extra": "CY=FY"},
    {"match": r"(?i)ë§ˆì´í¬ë¡œì†Œí”„íŠ¸|microsoft|msft", "fy_end": "6ì›” 30ì¼", "cy_aligned": False, "extra": "FY=7~6ì›”"},
    {"match": r"(?i)ì‚¼ì„±ì „ì|samsung", "fy_end": "12ì›” 31ì¼", "cy_aligned": True, "extra": "ì—°ê²° ê¸°ì¤€ 12ì›” ê²°ì‚°"},
    {"match": r"(?i)sk\s*hynix|ì—ìŠ¤ì¼€ì´í•˜ì´ë‹‰ìŠ¤|í•˜ì´ë‹‰ìŠ¤|SKí•˜ì´ë‹‰ìŠ¤", "fy_end": "12ì›” 31ì¼", "cy_aligned": True, "extra": "12ì›” ê²°ì‚°"},
]

def fy_cy_note(company_name: str):
    for item in FY_CY_INFO:
        if re.search(item["match"], str(company_name), flags=re.I):
            aligned = "ì˜ˆ (CY=FY)" if item["cy_aligned"] else "ì•„ë‹ˆì˜¤"
            extra = f" ({item.get('extra','')})" if item.get('extra') else ""
            return f"â€¢ **{company_name}** â€” FY ê²°ì‚°ì›”: **{item['fy_end']}**, CYì™€ ì¼ì¹˜: **{aligned}**{extra}"
    return f"â€¢ **{company_name}** â€” FY ê²°ì‚°ì›”: **ë¯¸ìƒ** (ë°ì´í„°ì…‹ ê¸°ì¤€: CY=FY ê°€ì •)"

# ================== ì¬ë¬´ì§€í‘œ ìŠ¤íƒ€ì¼ ìš”ì•½ ==================
FIN_STYLE_INFO = [
    {
        "match": r"(?i)ì—”ë¹„ë””ì•„|nvidia",
        "bullets": [
            "ë³´ê³  ì„¸ê·¸ë¨¼íŠ¸: **Compute & Networking / Graphics**",
            "íŠ¹ì´: ìˆ˜ì¶œê·œì œÂ·ì¬ê³ (ì„ ê¸‰Â·LTA) ì½”ë©˜íŠ¸ ë¹ˆë²ˆ"
        ]
    },
    {
        "match": r"(?i)ì•„ë§ˆì¡´|amazon",
        "bullets": [
            "ì„¸ê·¸ë¨¼íŠ¸: **NA / International / AWS**",
            "íŠ¹ì´: **FCF(ë¦¬ìŠ¤Â·ê¸ˆìœµì˜ë¬´ ì°¨ê° ë²„ì „)** ë³‘í–‰ ê³µì‹œ"
        ]
    },
    {
        "match": r"(?i)ì•ŒíŒŒë²³|êµ¬ê¸€|alphabet|google",
        "bullets": [
            "ì„¸ê·¸ë¨¼íŠ¸: **Google Services / Google Cloud / Other Bets**",
            "íŠ¹ì´: Cloud í‘ì ì§€ì†ì„±, ex-TAC ê´€ì "
        ]
    },
    {
        "match": r"(?i)ë©”íƒ€|meta",
        "bullets": [
            "ì„¸ê·¸ë¨¼íŠ¸: **Family of Apps / Reality Labs**",
            "íŠ¹ì´: RL ëŒ€ê·œëª¨ íˆ¬ìÂ·ì ì"
        ]
    },
    {
        "match": r"(?i)ë§ˆì´í¬ë¡œì†Œí”„íŠ¸|microsoft|msft",
        "bullets": [
            "ì„¸ê·¸ë¨¼íŠ¸: **P&BP / Intelligent Cloud / MPC**",
            "íŠ¹ì´: ìƒìˆ˜í™˜ìœ¨ ì§€í‘œ ë³‘í–‰"
        ]
    },
    {
        "match": r"(?i)ì‚¼ì„±ì „ì|samsung",
        "bullets": [
            "ì‚¬ì—…ë¶€: **DX / DS / SDC / Harman**",
            "íŠ¹ì´: ì—°ê²° ê¸°ì¤€(ë°˜ë„ì²´+ëª¨ë°”ì¼/ê°€ì „ í¬í•¨)"
        ]
    },
    {
        "match": r"(?i)sk\s*hynix|ì—ìŠ¤ì¼€ì´í•˜ì´ë‹‰ìŠ¤|í•˜ì´ë‹‰ìŠ¤|SKí•˜ì´ë‹‰ìŠ¤",
        "bullets": [
            "íšŒê³„: **K-IFRS(ì—°ê²°)**, ë©”ëª¨ë¦¬ **Pure-play**",
            "íŠ¹ì´: ì—…í™© ë¯¼ê° â€” ë¶€ì±„Â·ìˆœë¶€ì±„ë¹„ìœ¨ ê°±ì‹  ì¦ìŒ"
        ]
    },
    {
        "match": r"(?i)amd|advanced micro devices",
        "bullets": [
            "ì„¸ê·¸ë¨¼íŠ¸: **Data Center / Client / Gaming / Embedded**",
            "íŠ¹ì´: Intel ëŒ€ë¹„ ì‹œì¥ì ìœ ìœ¨, AI/HPC ì§„ì¶œ"
        ]
    },
]

def fin_style_note(company_name: str) -> str:
    for item in FIN_STYLE_INFO:
        if re.search(item["match"], str(company_name), flags=re.I):
            bullets = "\n".join([f"   - {b}" for b in item["bullets"]])
            return f"**{company_name} â€“ ì¬ë¬´ì§€í‘œ ìŠ¤íƒ€ì¼**\n{bullets}"
    return f"**{company_name} â€“ ì¬ë¬´ì§€í‘œ ìŠ¤íƒ€ì¼**\n   - (ì¤€ë¹„ëœ ìš”ì•½ ì—†ìŒ)"

# ================== Streamlit UI ==================
st.set_page_config(page_title="PDF to Visualization - ì™„ì „ í†µí•© ì•±", layout="wide")
st.title("ğŸ“Š ì‹œì¥ ë°ì´í„° ìë™ ë¶„ì„")

st.markdown("""
### ğŸš€ ì–´ë–¤ ì•±ì¸ê°€ìš”?
- ì´ ì•±ì€ ë³µì¡í•œ PDF ì¬ë¬´/ì‹¤ì  ë¦¬í¬íŠ¸ë¥¼ í•œ ëˆˆì— ë³´ê¸° ì¢‹ì€ ì½¤ë³´ ì°¨íŠ¸(ë§‰ëŒ€+êº¾ì€ì„ )ë¡œ ì‹œê°í™”í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
""")

# ================== ë‹¨ê³„ 1: PDF ì²˜ë¦¬ (ì™„ì „ ê¸°ëŠ¥) ==================
st.header("ğŸ“„ PDF ì—…ë¡œë“œ")

uploaded_pdf_files = st.file_uploader(
    "PDF ë³´ê³ ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)", type=["pdf"], accept_multiple_files=True
)

if uploaded_pdf_files:
    max_workers = 12

    if st.button("ğŸš€ PDF ì²˜ë¦¬ ì‹œì‘ (ì „ì²´ ê¸°ëŠ¥)"):
        all_processed_dfs = []
        all_errors = []
        all_missing_segments = {}
        all_csv_files = []  # (filename, bytes) â†’ ZIPìš©
        all_summaries = {}

        predefined_companies = [c.lower() for c in company_segments.keys()]
        company_name_map = {c.lower(): c for c in company_segments.keys()}

        overall_progress = st.progress(0.0)
        total_files = len(uploaded_pdf_files)

        for file_idx, uploaded_pdf_file in enumerate(uploaded_pdf_files, start=1):
            file_name = uploaded_pdf_file.name
            file_name_lower = file_name.lower()

            company_name = None
            for c_lower in predefined_companies:
                if c_lower in file_name_lower:
                    company_name = company_name_map[c_lower]
                    break

            if company_name is None:
                if any(keyword in file_name_lower for keyword in ['amd', 'advanced micro devices']):
                    company_name = "AMD"
                else:
                    company_name = re.sub(r"\.pdf$", "", file_name, flags=re.IGNORECASE).strip()

            company_safe = safe_filename(company_name)

            if is_amd_company(company_name):
                st.info(f"ğŸ“ ì²˜ë¦¬ ì¤‘: **{file_name}** â†’ ê¸°ì—…ëª…: **{company_name}**")

            with st.spinner(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... ({file_name})"):
                uploaded_pdf_file.seek(0)
                pages = extract_text_from_pdf(uploaded_pdf_file)
                pdf_text = "\n".join(pages)

            st.write(f"ì´ {len(pages)}í˜ì´ì§€. GPT í‘œ ì¶”ì¶œ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")

            with st.spinner(f"'{file_name}' í•µì‹¬ ìš”ì•½ ìƒì„± ì¤‘..."):
                summary_result = get_summary_from_pdf(pdf_text, client, MODEL_NAME)
                all_summaries[company_name] = summary_result

            results = []
            per_file_progress = st.progress(0.0)
            with concurrent.futures.ThreadPoolExecutor(max_workers=int(max_workers)) as executor:
                futures = {executor.submit(process_single_page, i, page_text): i for i, page_text in enumerate(pages)}
                for count, future in enumerate(concurrent.futures.as_completed(futures), 1):
                    i, output = future.result()
                    results.append((i, output))
                    per_file_progress.progress(count / len(pages))
                    time.sleep(0.05)

            results.sort(key=lambda x: x[0])

            file_extracted_dfs = []
            pages_dict_for_preview = {}

            for i, output in results:
                page_no = i + 1
                out = (output or "").strip()
                if out.upper() == "NONE":
                    continue
                if out.startswith("ERROR"):
                    all_errors.append(f"'{file_name}' p.{page_no} ì˜¤ë¥˜: {out}")
                    continue

                out = strip_code_fences(out)
                table_texts = split_multiple_tables(out)

                for t_idx, table_text in enumerate(table_texts, start=1):
                    try:
                        df = pd.read_csv(io.StringIO(table_text), sep=";", index_col=0)
                        if df.empty:
                            raise ValueError("ë¹ˆ DataFrame")

                        # ì½ìë§ˆì ì»¬ëŸ¼ ì •ê·œí™” + ACTUAL ì²˜ë¦¬ + suffixí–‰ ì ‘ê¸° + ë¶ˆí•„ìš” revenue í–‰ ì œê±° + ì›ì¹˜ ì•ŠëŠ” í–‰ ì œê±° (ë‚´ë¶€ìš©)
                        df = normalize_df_columns(df)
                        df = handle_actual_columns(df)
                        df = fold_month_suffix_rows(df)
                        df = drop_unwanted_revenue_rows(df)
                        df = remove_unwanted_rows(df)

                        # ë‚´ë¶€ ì²˜ë¦¬ìš© DF ë³´ê´€
                        file_extracted_dfs.append(df)

                        # === í‘œì‹œ/ë‹¤ìš´ë¡œë“œëŠ” ì˜ˆì¸¡ì¹˜ F ìœ ì§€í•œ ê°„ëµ í¬ë§· ===
                        display_df = to_compact_columns(df, keep_F=True)
                        pages_dict_for_preview.setdefault(page_no, []).append(display_df)

                        # ZIPìš© CSV ì €ì¥(ì˜ˆì¸¡ì¹˜ F ë³´ì„)
                        csv_bytes = display_df.to_csv(index=False).encode("utf-8")
                        fname = f"{company_safe}_page{page_no}_table{t_idx}.csv"
                        all_csv_files.append((fname, csv_bytes))
                    except Exception as e:
                        all_errors.append(
                            f"'{file_name}' p.{page_no} í‘œ {t_idx} CSV íŒŒì‹± ì‹¤íŒ¨: {e}\nì›ë¬¸:\n{table_text[:4000]}"
                        )

            if pages_dict_for_preview:
                with st.expander(f"{file_name} â€“ ì¶”ì¶œ í‘œ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                    for pno in sorted(pages_dict_for_preview.keys()):
                        st.markdown(f"**í˜ì´ì§€ {pno}** â€“ í‘œ {len(pages_dict_for_preview[pno])}ê°œ")
                        for k, dfv in enumerate(pages_dict_for_preview[pno], start=1):
                            st.dataframe(dfv)
                            csv_bytes = dfv.to_csv(index=False).encode("utf-8")
                            st.download_button(
                                label=f"ğŸ“¥ {company_safe}_page{pno}_table{k}.csv ë‹¤ìš´ë¡œë“œ",
                                data=csv_bytes,
                                file_name=f"{company_safe}_page{pno}_table{k}.csv",
                                mime="text/csv",
                                key=f"dl_{company_safe}_{pno}_{k}",
                            )

            if file_extracted_dfs or is_amd_company(company_name):
                st.success(f"'{file_name}'ì—ì„œ ì´ {len(file_extracted_dfs)}ê°œ í‘œ ì¶”ì¶œ")

                # ì™„ì „í•œ ì „ì²˜ë¦¬ ì‹¤í–‰
                result_df_single, errors_single, missing_segments_single = process_extracted_dfs(
                    file_extracted_dfs, company_name
                )
                if errors_single:
                    all_errors.extend(errors_single)

                if result_df_single is not None and not result_df_single.empty:
                    result_df_single.insert(0, "Company", company_name)
                    all_processed_dfs.append(result_df_single)
                    with st.expander(f"{file_name} â€“ ì „ì²˜ë¦¬ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        st.dataframe(to_compact_columns(result_df_single, keep_F=True))
                else:
                    st.info(f"âš ï¸ '{file_name}' ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

                if missing_segments_single:
                    all_missing_segments[company_name] = missing_segments_single
            else:
                st.info(f"âš ï¸ '{file_name}'ì—ì„œ ìœ íš¨í•œ í‘œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            overall_progress.progress(file_idx / total_files)
            time.sleep(0.05)

        # ì „ì²´ ZIP
        if all_csv_files:
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, "w", zipfile.ZIP_DEFLATED) as zf:
                for fname, data_bytes in all_csv_files:
                    zf.writestr(fname, data_bytes)
            zip_buffer.seek(0)


        # ìµœì¢… í†µí•©
        st.header("â€” ìµœì¢… í†µí•© ê²°ê³¼ â€”")
        if all_processed_dfs:
            final_integrated_df = pd.concat(all_processed_dfs, axis=0, ignore_index=False)

            # ë³´ê³ ì„œì˜ Revised/Previous/Chg./CHG í¬í•¨ ì—´ ì œê±° (ë§ˆì§€ë§‰ ì•ˆì „ì¥ì¹˜)
            cols_to_drop = [
                col for col in final_integrated_df.columns
                if any(w in str(col).upper() for w in ["REVISED", "PREVIOUS", "CHG.", "CHG", "CHANGE", "2025E.1", "YR", "YR.1", "YR.2"])
                or str(col).upper().endswith("_CHG")  # _CHGë¡œ ëë‚˜ëŠ” ì»¬ëŸ¼ ì¶”ê°€ ì²´í¬
            ]

            if cols_to_drop:
                final_integrated_df = final_integrated_df.drop(columns=cols_to_drop, errors="ignore")

            # ACTUAL ì»¬ëŸ¼ ìµœì¢… ì²˜ë¦¬
            final_integrated_df = handle_actual_columns(final_integrated_df)

            # ì›ì¹˜ ì•ŠëŠ” í–‰ ìµœì¢… ì œê±°
            final_integrated_df = remove_unwanted_rows(final_integrated_df)

            if "Company" in final_integrated_df.columns:
                cols = final_integrated_df.columns.tolist()
                cols.insert(0, cols.pop(cols.index("Company")))
                final_integrated_df = final_integrated_df[cols]

            display_final = to_compact_columns(final_integrated_df, keep_F=True)

            with st.expander("í†µí•© CSV ì „ì²´ ë³´ê¸°"):
                st.dataframe(display_final)

            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ì‹œê°í™”ìš©)
            st.session_state.final_df = final_integrated_df
            st.session_state.display_df = display_final
            st.session_state['all_summaries'] = all_summaries

            if all_errors:
                st.error("âŒ ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜:")
                for e in all_errors:
                    st.error(e)

            if all_missing_segments:
                st.warning("âš ï¸ ë‹¤ìŒ ê¸°ì—…ì€ ì‚¬ì „ ê¸°ì¤€ ëŒ€ë¹„ ëˆ„ë½ ì„¸ê·¸ë¨¼íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤:")
                for company, missing in all_missing_segments.items():
                    st.warning(f"â€¢ {company}: {', '.join(missing)}")

            csv_bytes = display_final.to_csv(sep=",", encoding="utf-8-sig").encode("utf-8-sig")
            st.download_button(
                label="ğŸ“¥ ìµœì¢… í†µí•© CSV ë‹¤ìš´ë¡œë“œ",
                data=csv_bytes,
                file_name="all_pdfs_integrated_complete.csv",
                mime="text/csv",
            )
        else:
            st.info("âš ï¸ í†µí•©í•  ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            if all_errors:
                st.error("âŒ ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜:")
                for e in all_errors:
                    st.error(e)

# ================== ë‹¨ê³„ 2: ì‹œê°í™” ==================

# PDFì—ì„œ ì¶”ì¶œí•œ ë°ì´í„°ê°€ ìˆê±°ë‚˜ ë³„ë„ CSV ì—…ë¡œë“œ
viz_df = None
display_for_viz = None

if 'final_df' in st.session_state and 'display_df' in st.session_state:
    viz_df = st.session_state.final_df
    display_for_viz = st.session_state.display_df

if viz_df is not None and not viz_df.empty:
    # ë°ì´í„° ë³€í™˜ (ê°€ë¡œí˜• â†’ ì„¸ë¡œí˜•)
    try:
        long_df = tidy_long(display_for_viz)

        if long_df.empty:
            st.warning("ë³€í™˜ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            st.stop()

        # ë°ì´í„° ê·¸ë£¹í™”
        long_df = long_df.groupby(['company', 'segment', 'ì‹œì '], as_index=False)['value'].sum()

        st.subheader("ğŸ“Š 2ë‹¨ê³„: ì‹œê°í™”")

        # ì‹œì  ìœ í˜• ì„ íƒ
        period_type = st.radio("ì‹œì  ìœ í˜• ì„ íƒ", ["ì—°ë„ë³„", "ë¶„ê¸°ë³„"], horizontal=True)

        # ë°ì´í„° í•„í„°ë§
        if period_type == "ì—°ë„ë³„":
            df_show = long_df[~long_df['ì‹œì '].str.contains('Q', na=False)]
        else:
            df_show = long_df[long_df['ì‹œì '].str.contains('Q', na=False)]

        if df_show.empty:
            st.warning(f"{period_type} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # ê¸°ì—… ë° ì§€í‘œ ì„ íƒ
        companies = sorted(df_show['company'].dropna().unique())
        metrics = sorted(df_show['segment'].dropna().unique())

        if len(companies) == 0 or len(metrics) < 2:
            st.warning("ì‹œê°í™”ì— í•„ìš”í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ìµœì†Œ 1ê°œ ê¸°ì—…, 2ê°œ ì§€í‘œ í•„ìš”)")
            st.stop()

        col1, col2 = st.columns(2)

        with col1:
            sel_companies = st.multiselect("ê¸°ì—… ì„ íƒ", companies, default=companies[:3] if len(companies) > 3 else companies)
            bar_metric = st.selectbox("Bar ì§€í‘œ ì„ íƒ", metrics)

        with col2:
            line_candidates = [m for m in metrics if m != bar_metric]
            if line_candidates:
                line_metric = st.selectbox("Line ì§€í‘œ ì„ íƒ", line_candidates)
            else:
                st.error("Line ì§€í‘œë¡œ ì‚¬ìš©í•  ë‹¤ë¥¸ ì§€í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()

        if sel_companies and bar_metric and line_metric:
            # ë°ì´í„° í•„í„°ë§
            mask = df_show['company'].isin(sel_companies) & df_show['segment'].isin([bar_metric, line_metric])
            valid_periods = (
                df_show[mask]
                .groupby('ì‹œì ')['value']
                .sum()
                .loc[lambda s: (s.notna()) & (s != 0)]
                .index
            )
            df_filtered = df_show[df_show['ì‹œì '].isin(valid_periods)]

            # xì¶• ì •ë ¬
            periods = set(df_filtered['ì‹œì '])
            if period_type == "ë¶„ê¸°ë³„":
                x_values = sorted(periods, key=quarter_sort_key)
            else:
                x_values = sorted(periods, key=year_sort_key)

            # ê³ ê¸‰ ì°¨íŠ¸ ìƒì„±
            fig = go.Figure()

            for comp in sel_companies:
                # ê¸°ì—…ë³„ ìƒ‰ìƒ ì§€ì • (í™•ì¥)
                comp_lower = comp.lower()
                if "hynix" in comp_lower or "í•˜ì´ë‹‰ìŠ¤" in comp_lower:
                    base_color = "#FF0000"  # SKí•˜ì´ë‹‰ìŠ¤: ë¹¨ê°„ìƒ‰
                elif "samsung" in comp_lower or "ì‚¼ì„±ì „ì" in comp_lower:
                    base_color = "#1428A0"  # ì‚¼ì„±ì „ì: íŒŒë€ìƒ‰
                elif "amd" in comp_lower:
                    base_color = "#000000"  # AMD: ê²€ì€ìƒ‰
                elif "amazon" in comp_lower or "ì•„ë§ˆì¡´" in comp_lower:
                    base_color = "#FF9900"  # Amazon: ì˜¤ë Œì§€ìƒ‰
                elif "nvidia" in comp_lower or "ì—”ë¹„ë””ì•„" in comp_lower:
                    base_color = "#76B900"  # NVIDIA: ë…¹ìƒ‰
                elif "google" in comp_lower or "alphabet" in comp_lower:
                    base_color = "#4285F4"  # Google: íŒŒë€ìƒ‰
                elif "meta" in comp_lower:
                    base_color = "#1877F2"  # Meta: íŒŒë€ìƒ‰
                elif "microsoft" in comp_lower:
                    base_color = "#00BCF2"  # Microsoft: í•˜ëŠ˜ìƒ‰
                else:
                    base_color = "#808080"  # ê·¸ ì™¸: íšŒìƒ‰

                r = int(base_color[1:3], 16)
                g = int(base_color[3:5], 16)
                b = int(base_color[5:], 16)
                bar_color = f"rgba({r},{g},{b},0.25)"

                # Bar ì°¨íŠ¸
                bar_df = df_filtered[(df_filtered['company']==comp) & (df_filtered['segment']==bar_metric)]
                if not bar_df.empty:
                    s = bar_df.set_index('ì‹œì ')['value']
                    xs = [x for x in x_values if x in s.index]
                    ys = [s.get(x, None) for x in xs]
                    fig.add_trace(go.Bar(
                        x=xs, y=ys,
                        name=f"{comp} â€“ {bar_metric}",
                        marker_color=bar_color,
                        yaxis='y',
                        width=0.35
                    ))

                # Line ì°¨íŠ¸
                line_df = df_filtered[(df_filtered['company']==comp) & (df_filtered['segment']==line_metric)]
                if not line_df.empty:
                    s = line_df.set_index('ì‹œì ')['value']
                    xs = []
                    ys = []
                    for x in x_values:
                        if x in s.index:
                            val = s.get(x, None)
                            # ê°’ì´ ìˆê³  0ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
                            if pd.notna(val) and val != 0:
                                xs.append(x)
                                ys.append(val)

                    # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì°¨íŠ¸ ì¶”ê°€
                    if xs and ys:
                        fig.add_trace(go.Scatter(
                            x=xs, y=ys,
                            name=f"{comp} â€“ {line_metric}",
                            yaxis='y2',
                            mode='lines+markers',
                            marker=dict(color=base_color, size=8),
                            line=dict(color=base_color, width=3),
                            connectgaps=False  # ë¹ˆ ê°’ ì‚¬ì´ë¥¼ ì—°ê²°í•˜ì§€ ì•ŠìŒ
                        ))

            # ê³ ê¸‰ ì°¨íŠ¸ ë ˆì´ì•„ì›ƒ
            fig.update_layout(
                title=f"ğŸ¢ {period_type} ê¸°ì—…ë³„ ì§€í‘œ ë¹„êµ ({bar_metric} vs {line_metric})",
                barmode='group',
                bargap=0.6,
                yaxis=dict(
                    title=dict(text=bar_metric, font=dict(size=14)),
                    side='left'
                ),
                yaxis2=dict(
                    title=dict(text=line_metric, font=dict(size=14)),
                    overlaying='y',
                    side='right',
                    showgrid=False
                ),
                xaxis=dict(
                    title=dict(text="ì‹œì ", font=dict(size=14)),
                    type="category",
                    categoryorder="array",
                    categoryarray=x_values,
                    tickangle=-45
                ),
                legend=dict(orientation="h", y=-0.15, x=0.5, xanchor='center'),
                height=700,
                font=dict(size=12),
                plot_bgcolor='rgba(248,249,250,0.8)',
                paper_bgcolor='white'
            )

            st.plotly_chart(fig, use_container_width=True)


            if sel_companies and st.session_state.get('all_summaries'):
                st.markdown("---")
                st.header("ğŸ“„ ë¶„ì„ ë³´ê³ ì„œ ìš”ì•½")

                for Company in sel_companies:
                    if Company in st.session_state['all_summaries']:
                        summary_text = st.session_state['all_summaries'][Company]
                        main_summary, detail_summaries, outlier_summaries = parse_summary_text_with_delta(summary_text)

                        with st.expander(f"{Company} ë¶„ì„ ìš”ì•½"):

                            # Markdown í…Œì´ë¸” ìƒì„± (1ë²ˆ ì½”ë“œì™€ ë™ì¼)
                            table_markdown = "<table>"

                            if main_summary:
                                table_markdown += f"<tr><td style='border: 1px solid #ddd; padding: 8px; width: 20%; font-weight: bold;'>í•µì‹¬ ìš”ì•½</td><td style='border: 1px solid #ddd; padding: 8px; width: 80%;'>{main_summary}</td></tr>"

                            if detail_summaries:
                                table_markdown += f"<tr><td rowspan='{len(detail_summaries)}' style='border: 1px solid #ddd; padding: 8px; width: 20%; font-weight: bold; vertical-align: top;'>ì£¼ìš” ì§€í‘œ</td>"
                                for i, s in enumerate(detail_summaries):
                                    s_styled = s.replace('ì¦ê°€', '<span style="color: #0000FF;">ì¦ê°€</span>').replace('ê°ì†Œ', '<span style="color: #FF0000;">ê°ì†Œ</span>')
                                    if i == 0:
                                        table_markdown += f"<td style='border: 1px solid #ddd; padding: 8px; width: 80%;'>{s_styled}</td></tr>"
                                    else:
                                        table_markdown += f"<tr><td style='border: 1px solid #ddd; padding: 8px; width: 80%;'>{s_styled}</td></tr>"

                            if outlier_summaries:
                                table_markdown += f"<tr><td rowspan='{len(outlier_summaries)}' style='border: 1px solid #ddd; padding: 8px; width: 20%; font-weight: bold; vertical-align: top;'>ì´ìƒì¹˜ ë¶„ì„</td>"
                                for i, s in enumerate(outlier_summaries):
                                    s_styled = s.replace('ì¦ê°€', '<span style="color: #0000FF;">ì¦ê°€</span>').replace('ê°ì†Œ', '<span style="color: #FF0000;">ê°ì†Œ</span>')
                                    if i == 0:
                                        table_markdown += f"<td style='border: 1px solid #ddd; padding: 8px; width: 80%;'>{s_styled}</td></tr>"
                                    else:
                                        table_markdown += f"<tr><td style='border: 1px solid #ddd; padding: 8px; width: 80%;'>{s_styled}</td></tr>"

                            table_markdown += "</table>"

                            st.markdown(table_markdown, unsafe_allow_html=True)
                            st.markdown("---")
                    else:
                        st.info(f"âš ï¸ {Company}ì— ëŒ€í•œ ìš”ì•½ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            st.header("ì°¸ê³ ")
            # ë°ì´í„° í…Œì´ë¸” í‘œì‹œ
            with st.expander("ğŸ“‹ ì°¨íŠ¸ ë°ì´í„° í™•ì¸"):
                chart_data = df_filtered[df_filtered['company'].isin(sel_companies) &
                                       df_filtered['segment'].isin([bar_metric, line_metric])]
                pivot_data = chart_data.pivot_table(
                    index=['company', 'segment'],
                    columns='ì‹œì ',
                    values='value',
                    fill_value=''
                )
                st.dataframe(pivot_data)

                # ì°¨íŠ¸ ë°ì´í„°ë„ ë‹¤ìš´ë¡œë“œ ê°€ëŠ¥í•˜ê²Œ
                chart_csv = pivot_data.to_csv(encoding="utf-8-sig").encode("utf-8-sig")
                st.download_button(
                    label="ğŸ“¥ ì°¨íŠ¸ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",
                    data=chart_csv,
                    file_name=f"chart_data_{bar_metric}_vs_{line_metric}.csv",
                    mime="text/csv"
                )

    except Exception as e:
        st.error(f"ì‹œê°í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.exception(e)

# ================== ì°¸ê³  ì •ë³´ Expanderë“¤ ==================
if 'final_df' in st.session_state or viz_df is not None:
    # í˜„ì¬ ë°ì´í„°ì—ì„œ ê¸°ì—… ëª©ë¡ ì¶”ì¶œ
    current_companies = []
    if 'final_df' in st.session_state:
        if 'Company' in st.session_state.final_df.columns:
            current_companies = st.session_state.final_df['Company'].dropna().unique().tolist()
    elif viz_df is not None and 'Company' in viz_df.columns:
        current_companies = viz_df['Company'].dropna().unique().tolist()

    if current_companies:
        with st.expander("ğŸ“š ì°¸ê³ : í˜„ì¬ ê¸°ì—…ë“¤ì˜ FY/CY ì •ë³´", expanded=False):
            notes = [fy_cy_note(c) for c in current_companies]
            st.markdown("\n\n".join(notes))
            st.caption("â€» ë¶„ê¸° í‘œê¸°(ì˜ˆ: 1Q2025F)ëŠ” ê° íšŒì‚¬ì˜ **FY ë¶„ê¸°** ê¸°ì¤€ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        with st.expander("ğŸ“Š ì°¸ê³ : í˜„ì¬ ê¸°ì—…ë“¤ì˜ ì¬ë¬´ì§€í‘œ ìŠ¤íƒ€ì¼", expanded=False):
            for c in current_companies:
                st.markdown(fin_style_note(c))
                st.markdown("---")
            st.caption("â€» ê° ê¸°ì—…ì˜ ë³´ê³  ë°©ì‹ê³¼ ì£¼ìš” KPIë¥¼ ì°¸ê³ í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.")
